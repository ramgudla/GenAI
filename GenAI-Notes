What is prompt engineering in the context of Large Language Models (LLMs)?
A: Iteratively refining the ask to elicit a desired response

What does the term "hallucination" refer to in the context of Language Large Models (LLMs)?
The phenomenon where the model generates factually incorrect information or unrelated content as if it were true

What is the role of temperature in the decoding process of a Large Language Model (LLM)?
To adjust the sharpness of probability distribution over vocabulary when selecting the next word

Fine-tuning modifies all parameters using labeled, task-specific data, whereas Parameter Efficient Fine-Tuning updates a few, new parameters also with labeled, task-specific data.

What does in-context learning in Large Language Models involve?
Conditioning the model with task-specific instructions or demonstrations

Temperature:
For model to be deterministic : 0 - selects blue (the word with the highest probability)
For model to be creative: > 0  - may select any other words
When temperature is increased, the distribution is flattened over all words

The sky is blue ---
blue 0.45,  the limit 0.25, red 0.3, water 0.01, tarnish 0.02

Top k : selects top k tokens in sorted order of probabilities.
Top p: selects top tokens based on the sum of their probabilities.
       Top p = .75 means, exclude the bottom .25 tokens.

Stop sequence: String that tells model to stop generating output content.
               If a period (.) is used as stop sequence, the model stops generating output at the end of the first sentence even if the no.of tokens limit is much higher.

Frequency and Presence penalties: To get output to be less repetitive text.

Show Likelihood: Every time a new token is generated, a number between -15 and 0 is assigned to all tokens.

Types of models: Generation Models, Summarization Models, Embedding Models
Model does: Text Generation, Text Extraction, Text Classification.

Embeddings are numerical representations of a piece of text converted to number sequences.

In-context learning: prompting an LLM with instructions and or demonstrations of the task it is meant to complete.
k-shot prompting (few-shot prompting): explicitly providing k examples of the intended task in the prompt.

Prompting strategies:
chain-of-thought: provide examples. includes a reasoning step.

Zero-shot chain-of-thought: apply chain-of-thought prompting without providing examples.

Prompt Engineering vs Fine-tuning vs RAG

. . .

The busy person's intro to LLMs
===============================
1.https://karpathy.ai/zero-to-hero.html
   https://www.youtube.com/watch?v=zjkBMFhNj_g
   https://www.youtube.com/watch?v=kCc8FmEb1nY
   https://www.youtube.com/watch?v=zduSFxRajkE

GenAI: build, train and infer LLM
=================================
1. https://blog.miguelgrinberg.com/post/how-llms-work-explained-without-math
2. https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/
3. https://www.omrimallis.com/posts/understanding-how-llm-inference-works-with-llama-cpp/
4. https://pyimagesearch.com/2024/08/26/llama-cpp-the-ultimate-guide-to-efficient-llm-inference-and-applications/
5. https://www.pluralsight.com/resources/blog/data/how-build-large-language-model
   https://towardsai.net/p/artificial-intelligence/build-your-own-large-language-model-llm-from-scratch-using-pytorch
   https://blog.stackademic.com/what-i-learned-from-creating-a-large-language-model-from-scratch-dd9a4dc6a73d

DeepMind.ai
==========
1. https://github.com/MalayAgr/generative-ai-with-llms-notes

Run LLMs on Your CPU with Llama.cpp
===================================
$ pip install --force-reinstall --ignore-installed --no-cache-dir llama-cpp-python
1. https://awinml.github.io/llm-ggml-python/
2. https://github.com/ggerganov/llama.cpp/discussions/4531

llama.cpp performs the following steps:
It initializes a llama context from the gguf file using the llama_init_from_file function. This function reads the header and the body of the gguf file and creates a llama context object, which contains the model information and the backend to run the model on (CPU, GPU, or Metal).
It tokenizes the input text using the llama_tokenize function. This function converts the input text into a sequence of tokens based on the tokenizer specified in the gguf file header. The tokens are stored in an array of llama tokens, which are integers that represent the token IDs.
It generates the output text using the llama_generate function. This function takes the input tokens and the llama context as arguments and runs the model on the backend. It uses the computation graph specified in the gguf file header to perform the forward pass of the model and calculate the next token probabilities. It then samples the next token from the probability distribution and appends it to the output tokens. It repeats this process until the end-of-text token or the maximum number of tokens is reached. The output tokens are stored in another array of llama tokens.
It detokenizes the output text using the llama_detokenize function. This function converts the output tokens into a string of text based on the tokenizer specified in the gguf file header. It handles the special tokens, such as the end-of-text token, the padding token, and the unknown token, and returns the final output text.

Ollama:
1. https://www.datacamp.com/tutorial/run-llama-3-locally


Oracel Vector Store
==================
https://luca-bindi.medium.com/oracle23ai-simplifies-rag-implementation-for-enterprise-llm-interaction-in-enterprise-solutions-d865dacdd1ed

https://github.com/ou-developers/ou-generativeai-pro/tree/main/demos

https://ryotayamanaka.medium.com/vector-search-in-oracle-23ai-cac685832d69

Run Oracle Database 23ai locally
================================
https://ronekins.com/2024/07/02/run-oracle-database-23ai-free-on-mac-computers-with-apple-silicon/

docker run -d \
  --name ora23ai \
  -p 1521:1521 \
  --mount source=oradata,target=/opt/oracle/oradata \
  container-registry.oracle.com/database/free

docker exec -it ora23ai ./setPassword.sh Welcome1
sql system/Welcome1@//localhost:1521/FREEPDB1

docker logs ora23ai
docker exec -it ora23ai ./setPassword.sh Welcome1
docker port ora23ai
sql system/Welcome1@//localhost:1521/FREE
sql system/Welcome1@//localhost:1521/FREEPDB1
docker exec -it ora23ai sqlplus / as sysdba

If you see below error:
error: ORA-43853: VECTOR type cannot be used in non-automatic segment space management tablespace "SYSTEM"

do as below:
SELECT tablespace_name,  segment_space_management FROM dba_tablespaces;

grant dba to testuser;
CREATE TABLESPACE tbs1 DATAFILE 'tbs1_data.dbf' SIZE 500m;
ALTER USER testuser QUOTA UNLIMITED ON tbs1
